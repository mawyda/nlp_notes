# notes for nlp course

# TODO: Include the beginning notes related to formatted string literals as well as regex stuff.
Introduction to NLP:
Video tutorial
For many tasks, spaCy has only one method, and it is typically the most current and efficient algorithm available.
NLTK is older but less efficient.
Spacy is better for most use cases, because we ignore implementations/ algorithms.
Spacy does not have any models for sentiment analysis, which is typically easier to perform with NLTK.

Download and installation:
You have to pip install and then download the language library.

Lecture 15: What is natural language processing?
How to get an AI to understand human language.

spaCy Basics (Lecture 16)
install and setup:
$ pip install -U spacy
$ python -m spacy download en

A few things:
"U.S.", for example, is kept together (we call this a token).
Parts of speech (ex. Proper Noun) are recognized.

$ nlp = spacy.load('en_core_web_sm')
In nlp = spacy.load(''), nlp is the model (from the language lib, I believe).
We applied the model to the text, and named it doc.

There is a processing pipeline that occurs when we apply the model to the text
to create doc. This can be altered, I believe. But currently there is a
 tagger, parser, ner. Tokenizer is probably a default portion.

See what's in the pipeline with the pipeline attribute:
$ nlp.pipeline # Again, nlp is the spacy model

$ nlp.pip_names # Displays just the names, not the objects in memory.


Tokenization:
# methods/ atts:
token.text, token.pos_, token.dep_

First step is to break everything up into tokens.
Periods and large gaps of whitespace are assigned tokens.
The original text can easily be grabbed by print(doc).
# doc.__str__() is the text entered. I.e. just printing the token will show
it's text.

You can also index tokens:
doc[0]

Parts of speech tagging:
After splitting into tokens, we tag with parts of speech.
The proper noun is applied due to some statistical modelling: we assume because
most things that follow "the" are nouns.

You can get this for the token with .pos_ att:
$ token.pos_ # Part of speech

Dependencies:
Syntactic dependencies assigned to each token. The notebook provides a link for
 a good description of these things.

Found with the dep_ att:
doc[2].dep_ == ROOT

To see the full name of a tag use the explain() method
spacy.explain(doc[0].pos_)
spacy.explain(doc[0].dep_)

# Note that both parts of speech and dependencies are considered tags.

Additional token attributes:
See the table for more.
lemma_ is short for lemmatization, which returns the base form of the word.

Spans

You can slice the doc object to returns spans of test. This is of the tokens, not the individual
characters of the words themselves.
There is a Span function that also allows us to create span, but for now we will use the slicing method.
quote = doc2[5:20]
type(quote) # spacy.tokens.span.Span

Sentences:
Certain tokens inside a doc may receive the "start of sentence" tag. This can allow us to generate sentence segments.
doc2.sents # is attribute and returns an iterable generator

You can test if the token is the start of a sentence with:
token.is_sent_start attribute.


01 - Tokenization:

The first step is to break down the Doc object into component pieces - tokens.
Q?: Why is the tutorial showing a quoted string?
'"We're moving to LA!"'
-Does spacy need this to parse up the data? And why were unicode strings being
used in the beginning?

# Note: the print() statements end argument in
print(mystring, end='|') does this add a character to the end of each statement?
- In a single print() statement, it won't actually print until something else
is printed.

Tokens are the basic building blocks of the text. They do not lemmatize or
convert to word stems.
They merely break things up for us at the highest level.
Notice also that quotation marks and punctuation is captured as well.

Prefixes, Suffixes, and Infixes:
spaCy will isolate punctuation that does not form an integral part of the word ('re in we're)
So starting and ending punctuation will be given it's own token.
Something like the @ in email addresses or part of a numerical value will not be parsed and kept as
part of the whole token.
Hyphens in between words may receive their own token, however.
Even doubled up punctuation will be tokenized individually.

Units of measure (5km == '5', 'km') or dollar signs will be assigned their own token.
Times and dollar amounts will not.
Q?: Note that adding pm to 3:30pm token was '3:30pm'. Why didn't this get
broken up?

Exceptions:
Punctuation that exists as apart of a known abbreviation will be preserved.
So U.S. and St. will be one whole token.

Counting Tokens:
Doc objects have a set number of tokens, which you can get with len():
len(doc)

Counting Vocab entries
You can get the count of this, which is dependent on the language (model)
selected.
len(doc.vocab)

Tokens can be retrieved by index position and slice:
doc[4], or a group with
doc[3:10]
Does support negative indexing:
doc[-4:] # Last four.

Tokens cannot be reassigned:
Although doc objects are lists of tokens, that are not explicitly lists and do not support item reassignment.
So:
doc[3] = 'new word' will not work.
Which makes sense considering all of the attributes assigned to that token when
the doc object is created. This would throw off the rest of the schema.

Named Entities
These add another layer of context. The model recognizes that certain words are
organizational names, money, etc.
You can access this collection of thing (technically a tuple) through the DocObj's
ents property:
doc = nlp(some text)
doc.ents # returns tuple.

# Notice that the end argument of the print() statement takes place of the
\n that would print things on the next line.

Named entity recognition (NER) is an important ML tool applied to NLP.
More to follow.

Noun Chunks:
noun_chunks are another Doc Obj property.
These are flat phrases that a noun as their head.
These can be though of as the noun and the words that describe them.
# I imagine this is good for sentiment analysis...

You can iterate through these like ents above:
for chunk in doc.noun_chunks:
    print(chunk.text)

# Note: text is not necessary, but explicit. Also, there are additional chunk components
that will be looked at.

Built-in visualizers:

spaCy provides a built-in visualization tool called displaCy. It is
able to detect if you are working in a Jupyter Notebook (ie., by passing an
argument).
But if working outside, it will serve up an html page to do so, like:
displacy.serve(doc, style = n,...)
The style can be 'dep' for dependency and 'ent' for the entity recognizer.
spacy.displacy.serve(doc, style = 'ent'

To create the visualizations in a notebook, just use the argument
jupyter = True


02 - Stemming
Sometimes it helps in a search query to return multiple versions of a word.
Ex. boats, boating, boat.
The process of stemming may be considered crude, but it truncates letters
until the root/ word stem is reached. boat|ing
spaCy doesn't use this process and instead chooses to rely on lemmatiztion,
which we will look at later.
For now, we use nltk.

Porter Stemmer
One of the most common and effective stemming algorithm developed by Martin Porter.
Goes through a series of phases to find the word stem, using multiple rules
$$$
import nltk # Why import everything?
from nltk.stem.porter import *

p_stemmer = PorterStemmer()
words = ['run','runner','running','ran','runs','easily','fairly']
for word in words:
    print(word + ' --> ' + p_stemmer.stem(word))
$$$
Note that some of the words are returned to unusual roots.

Snowball Stemmer
Has multiple names, but is a slight improvement on the Porter Stemmer in terms of logic and speed.

from nltk.stem.snowball import SnowballStemmer
# Language required
s_stemmer = SnowballStemmer(language='english')
words = ['run','runner','running','ran','runs','easily','fairly']
for word in words:
    print(word + ' --> ' + s_stemmer.stem(word))

# Not a huge difference from the first stemmer in terms of what I would consider accuracy.

# Notes:
stemmers.stem() works on strings.

Stemming does have it's drawbacks where lemmatization might be an improvement.
For instance, a word may not be recognized as the correct part of speech:
phrase = 'I am meeting him tomorrow at the meeting.'
for word in phrase.split():
    print(word + ' --> ' + p_stemmer.stem(word))
# Both meetings will reduce to 'meet', but the latter one should be meeting
because that is the full noun.

Lemmatization -03
Looks beyond word reduction and applies a morphological analysis to words.
This is more than just truncating characters.
For example, the lemma of 'was' is 'be' and meeting may be meet or meeting depending
on it's use in a sentence).

# Note: Using spaCy for this stuff. This gets applied to each token.
Notice the difference between lemma and lemma_.

import spacy
nlp = spacy.load('en_core_web_sm')
doc1 = nlp(u"I am a runner running in a race because I love to run since I ran today")

# Notice that 'I' as the lemma_ of -PRON-, instead of a different lemma.

token.lemma
token.lemma_

Q?: What is the difference between the two? Without the underscore, a
hash number is returned.
With lemma_, the root word is returned.

Function to display lemmas

We write a function to display lemmas and other parts of the word.
Uses the doc as argument. i.e. nlp(text)

def show_lemmas(text):
    for token in test:
        # Returns the text, the Part of Speech, the lemma number, and the lemma
         itself.
        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')

# Note the left-align added to the lemma hash value.

Lemmatization does not reduce words down to their most basic synonym.
I.e. automobile does not become car.

Lemmatization does not categorize phrases.
# Not sure what that means or why it is important.

The basic point is that using spaCy's lemma tools, you can get the correct root
of the word, paying attention even to the token's part of speech.


Stop Words - 04:
Words like 'a' and 'the' that appear so frequently that they do not need to be
tagged as thoroughly as nouns, verbs and modifiers.

To show these:
nlp.Defaults.stop_words # type == set.

To check if a word is a stopword:
nlp.vocab['myself'].is_stop  # == True
nlp.vocab['mystery'].is_stop # == False

To add a stop word:
In this example, we want to add 'btw' to the stop words:
nlp.Defaults.stop_words.add('btw')
nlp.vocab['btw'],is_stop == True
# Always add stopwords as lower case.

To remove a stopword:
nlp.Defaults.stop_words.remove(<string>)
# Apparently it must also be removed from the lexeme. Why?
nlp.vocab[<string>].is_stop = False


05- Vocabulary and Matching.
In this section we;ll identify and label specific phrases that match patterns
we can define ourselves.

Rule-based Matching

spaCy offers a rule-matching tool called Matcher that allows you to build
a library of token patterns, then match those patterns against a Doc object to return
a list of found matches.
You can add multiple patterns to the matcher.
$ from spacy.matcher import Matcher
$ matcher = Matcher(nlp.vocab)

Here matcher is an object that pairs to the current Vocab object. We can add an remove specific named matches to matcher
as needed.

Creating patterns:
'solar power', for example, might appear as one word or two, with or without a hyphen.
We can match all three:
pattern1 = [{'LOWER': 'solarpower'}]
pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]
pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]

matcher.add('SolarPower', None, pattern1, pattern2, pattern3)

# What does LOWER stand for in the matching pattern?
The 2nd pattern looks for the two words that are adjacent to each other.
The third looks for one with punctuation inbetween the two words.
Finally, we pass them in to the matcher object, giving it the name SolarPower,
and setting callbacks to None (see below).

Applying the matcher to a Doc object
Do so by passing in the doc to the matcher object:
$ found = matcher(doc)
A list of three-tuples is returned, containing the ID for the match, with start & end tokens that map to the span
doc[start:end]

The match_id is the hash value of the string_ID, so:
nlp.vocab.strings[match_id] # returns SolarPower
# Note: this is the name that we applied to this match in the matcher.

Setting pattern options and quantifiers:

You can make token rules optional by passing an 'OP': '*' argument.
pattern1 = [{'LOWER': 'solarpower'}]
pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP': '*'}, {'LOWER': 'power'}]
# We now kill the old patterns
matcher.remove('SolarPower')
# And add the new set to the matcher.
matcher.add('SolarPower', None, pattern1, pattern2)

OP	Description
\!	Negate the pattern, by requiring it to match exactly 0 times
?	Make the pattern optional, by allowing it to match 0 or 1 times
\+	Require the pattern to match 1 or more times
\*	Allow the pattern to match zero or more times

# The above are regex like quantifiers we can use on the patterns.
# On the pattern above, we are saying there maybe a character between
# the two words.

Be careful with lemmas!
You can set the pattern to look for the lemmas of words, but _pos matters too.
So if you expect the lemma to be x, but the _pos changes that (powered as an adjective is still powered),
you will not catch everything intended.
# I'm not actually seeing the same as the book. Both instances are coming through as
lemma_ == 'power'

You can set the Lemma in the matcher by passing the key as 'LEMMA' (versus 'LOWER').

Other token attributes.
There are a variety of token attributes we can use to determine matching rules.

Attribute	Description
`ORTH`	The exact verbatim text of a token
`LOWER`	The lowercase form of the token text
`LENGTH`	The length of the token text
`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`	Token text consists of alphanumeric characters, ASCII characters, digits
`IS_LOWER`, `IS_UPPER`, `IS_TITLE`	Token text is in lowercase, uppercase, titlecase
`IS_PUNCT`, `IS_SPACE`, `IS_STOP`	Token is punctuation, whitespace, stop word
`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`	Token text resembles a number, URL, email
`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`	The token's simple and extended part-of-speech tag, dependency label, lemma, shape
`ENT_TYPE`	The token's entity label

Token wildcard:
You can pass an empty dictionary {} as wildcard to represent any token.
This pattern will cpature hashtags, for example:
[{'ORTH': '#'}, {}]

PhraseMatcher:

You can match on terminology lists instead of token patterns. This requires
making each phrase a doc obj (applying the nlp() object to it), and using a different
matcher object.

from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab)

phrase_list = ['voodoo economics', 'trickle-down economics']
# doc-ify these:
phrase_patterns = [nlp(text) for text in phrase_list]
# add these to the matcher now:
matcher.add('VoodooEconomics', None, *phrase_patterns)
# We name it, not sure what the None is for, and we pass the *args.

# Finally, build a list of matches:
matches = matcher(doc3)

The above will return a list of three-tuples that contains the the hash, start and end indices.

Viewing Matches:
You can view matches by passing a greater span to the doc and viewing from there.

Another way is to apply the senticizer to the Doc, then iterate thorugh the sentnces to the match point:

# Build a list of sentences
sents = [sent for sent in doc3.sents]
# You can then get the start and end of each sentence.
sent[0].start, sent[0].end
# Note, you can use indices that are past the start and end of the sentence.
# !!! Important !!!
The indices returned are relative to the doc, not the sentence. So,
if 20 and 35 are returned, using sents[1][35] will refer to the token in the doc, not the sentence.

# Iterate over the sentence list until the sentence end value exceeds a match start value.
# A generator would be useful here. For each sentence, for each match, find it's sentence.

#  why didn't this work?
for match in matches:
    for sent in sents:
        if match[1] < sent.end:
            print(sent)
            break

for match in matches:
    for sent in sents:
        if match[1] < sent.end:
            print(sent)
            break

























