# notes for nlp course

# TODO: Include the beginning notes related to formatted string literals as well as regex stuff.


spaCy Basics
install and setup:
$ pip install -U spacy
$ python -m spacy download

A few things:
U.S. is kept together (we call this a token)
Parts of speech (ex. Proper Noun) are recognized.

$ nlp = spacy.load('en_core_web_sm')
In nlp = spacy.load(''), nlp is the model (from the language lib, I believe).
We applied the model to the text, and named it doc.

There is a processing pipeline that occurs when we apply the model to the text
to create doc. This can be altered, I believe. But currently there is a
 tagger, parser, ner. Tokenizer is probably a default portion.

See what's in the pipeline with the pipeline attribute:
$ nlp.pipeline # Again, nlp is the spacy model

$ nlp.pip_names # Displays just the names, not the objects in memory.


Tokenization:
# methods/ atts:
token.text, token.pos_, token.dep_

First step is to break everything up into tokens.
Periods and large gaps of whitespace are assigned tokens.
The original text can easily be grabbed by print(doc).
# doc.__str__() is the text entered. I.e. just printing the token will show
it's text.

You can also index tokens:
doc[0]

Parts of speech tagging:
After splitting into tokens, we tag with parts of speech.
The proper noun is applied due to some statistical modelling: we assume because
most things that follow "the" are nouns.

You can get this for the token with .pos_ att:
$ token.pos_ # Part of speech

Dependencies:
Syntactic dependencies assigned to each token. The notebook provides a link for
 a good description of these things.

Found with the dep_ att:
doc[2].dep_ == ROOT

To see the full name of a tag use the explain() method
spacy.explain(doc[0].pos_)
spacy.explain(doc[0].dep_)

# Note that both parts of speech and dependencies are considered tags.

Additional token attributes:
See the table for more.
lemma_ is short for lemmatization, which returns the base form of the word.

Spans

You can slice the doc object to returns spans of test. This is of the tokens, not the individual
characters of the words themselves.
There is a Span function that also allows us to create span, but for now we will use the slicing method.
quote = doc2[5:20]
type(quote) # spacy.tokens.span.Span

Sentences:
Certain tokens inside a doc may receive the "start of sentence" tag. This can allow us to generate sentence segments.
doc2.sents # is attribute and returns an iterable generator

You can test if the token is the start of a sentence with:
token.is_sent_start attribute.


01 - Tokenization:

The first step is to break down the Doc object into component pieces - tokens.
Q?: Why is the tutorial showing a quoted string?
'"We're moving to LA!"'
-Does spacy need this to parse up the data? And why were unicode strings being
used in the beginning?

# Note: the print() statements end argument in
print(mystring, end='|') does this add a character to the end of each statement?
- In a single print() statement, it won't actually print until something else
is printed.

Tokens are the basic building blocks of the text. They do not lemmatize or
convert to word stems.
They merely break things up for us at the highest level.
Notice also that quotation marks and punctuation is captured as well.

Prefixes, Suffixes, and Infixes:
spaCy will isolate punctuation that does not form an integral part of the word ('re in we're)
So starting and ending punctuation will be given it's own token.
Something like the @ in email addresses or part of a numerical value will not be parsed and kept as
part of the whole token.
Hyphens in between words may receive their own token, however.
Even doubled up punctuation will be tokenized individually.

Units of measure (5km == '5', 'km') or dollar signs will be assigned their own token.
Times and dollar amounts will not.
Q?: Note that adding pm to 3:30pm token was '3:30pm'. Why didn't this get
broken up?

Exceptions:
Punctuation that exists as apart of a known abbreviation will be preserved.
So U.S. and St. will be one whole token.

Counting Tokens:
Doc objects have a set number of tokens, which you can get with len():
len(doc)

Counting Vocab entries
You can get the count of this, which is dependent on the language (model)
selected.
len(doc.vocab)

Tokens can be retrieved by index position and slice:
doc[4], or a group with
doc[3:10]
Does support negative indexing:
doc[-4:] # Last four.

Tokens cannot be reassigned:
Although doc objects are lists of tokens, that are not explicitly lists and do not support item reassignment.
So:
doc[3] = 'new word' will not work.
Which makes sense considering all of the attributes assigned to that token when
the doc object is created. This would throw off the rest of the schema.

Named Entities
These add another layer of context. The model recognizes that certain words are
organizational names, money, etc.
You can access this collection of thing (technically a tuple) through the DocObj's
ents property:
doc = nlp(some text)
doc.ents # returns tuple.

# Notice that the end argument of the print() statement takes place of the
\n that would print things on the next line.

Named entity recognition (NER) is an important ML tool applied to NLP.
More to follow.

Noun Chunks:
noun_chunks are another Doc Obj property.
These are flat phrases that a noun as their head.
These can be though of as the noun and the words that describe them.
# I imagine this is good for sentiment analysis...

You can iterate through these like ents above:
for chunk in doc.noun_chunks:
    print(chunk.text)

# Note: text is not necessary, but explicit. Also, there are additional chunk components
that will be looked at.

Built-in visualizers:

spaCy provides a built-in visualization tool called displaCy. It is
able to detect if you are working in a Jupyter Notebook (ie., by passing an
argument).
But if working outside, it will serve up an html page to do so, like:
displacy.serve(doc, style = n,...)
The style can be 'dep' for dependency and 'ent' for the entity recognizer.
spacy.displacy.serve(doc, style = 'ent'

To create the visualizations in a notebook, just use the argument
jupyter = True


02 - Stemming
Sometimes it helps in a search query to return multiple versions of a word.
Ex. boats, boating, boat.
The process of stemming may be considered crude, but it truncates letters
until the root/ word stem is reached. boat|ing
spaCy doesn't use this process and instead chooses to rely on lemmatiztion,
which we will look at later.
For now, we use nltk.

Porter Stemmer
One of the most common and effective stemming algorithm developed by Martin Porter.
Goes through a series of phases to find the word stem, using multiple rules
$$$
import nltk # Why import everything?
from nltk.stem.porter import *

p_stemmer = PorterStemmer()
words = ['run','runner','running','ran','runs','easily','fairly']
for word in words:
    print(word + ' --> ' + p_stemmer.stem(word))
$$$
Note that some of the words are returned to unusual roots.

Snowball Stemmer
Has multiple names, but is a slight improvement on the Porter Stemmer in terms of logic and speed.

from nltk.stem.snowball import SnowballStemmer
# Language required
s_stemmer = SnowballStemmer(language='english')
words = ['run','runner','running','ran','runs','easily','fairly']
for word in words:
    print(word + ' --> ' + s_stemmer.stem(word))

# Not a huge difference from the first stemmer in terms of what I would consider accuracy.

# Notes:
stemmers.stem() works on strings.

Stemming does have it's drawbacks where lemmatization might be an improvement.
For instance, a word may not be recognized as the correct part of speech:
phrase = 'I am meeting him tomorrow at the meeting.'
for word in phrase.split():
    print(word + ' --> ' + p_stemmer.stem(word))
# Both meetings will reduce to 'meet', but the latter one should be meeting
because that is the full noun.

Lemmatization -03
Looks beyond word reduction and applies a morphological analysis to words.
This is more than just truncating characters.
For example, the lemma of 'was' is 'be' and meeting may be meet or meeting depending
on it's use in a sentence).

# Note: Using spaCy for this stuff. This gets applied to each token.
Notice the difference between lemma and lemma_.

import spacy
nlp = spacy.load('en_core_web_sm')
doc1 = nlp(u"I am a runner running in a race because I love to run since I ran today")

# Notice that 'I' as the lemma_ of -PRON-, instead of a different lemma.

token.lemma
token.lemma_

Q?: What is the difference between the two? Without the underscore, a
hash number is returned.
With lemma_, the root word is returned.

Function to display lemmas

We write a function to display lemmas and other parts of the word.
Uses the doc as argument. i.e. nlp(text)

def show_lemmas(text):
    for token in test:
        # Returns the text, the Part of Speech, the lemma number, and the lemma
         itself.
        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')

# Note the left-align added to the lemma hash value.

Lemmatization does not reduce words down to their most basic synonym.
I.e. automobile does not become car.

Lemmatization does not categorize phrases.
# Not sure what that means or why it is important.

The basic point is that using spaCy's lemma tools, you can get the correct root
of the word, paying attention even to the token's part of speech.


Stop Words - 04:
Words like 'a' and 'the' that appear so frequently that they do not need to be
tagged as thoroughly as nouns, verbs and modifiers.

To show these:
nlp.Defaults.stop_words # type == set.

To check if a word is a stopword:
nlp.vocab['myself'].is_stop  # == True
nlp.vocab['mystery'].is_stop # == False

To add a stop word:
In this example, we want to add 'btw' to the stop words:
nlp.Defaults.stop_words.add('btw')
nlp.vocab['btw'],is_stop == True
# Always add stopwords as lower case.

To remove a stopword:
nlp.Defaults.stop_words.remove(<string>)
# Apparently it must also be removed from the lexeme. Why?
nlp.vocab[<string>].is_stop = False


05- Vocabulary and Matching.
In this section we;ll identify and label specifc phrases that match patterns
we can define ourselves.

Rule-based Matching















